{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducci칩n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este documento se presenta el desarrollo completo del proceso de entrenamiento y validaci칩n de un modelo dise침ado para clasificar imagenes marinas en buena y mala calidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image,  ImageOps\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import RandomSampler\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from PIL import ImageFile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU: NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU no disponible, usando CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299,299)),#Las transforma a imagenes de 299x299\n",
    "    transforms.ToTensor() #Transforma las imagenes a tensor\n",
    "])\n",
    "\n",
    "\n",
    "transform1 = transforms.Compose([ \n",
    "    transforms.RandomRotation(degrees=(60, 140)),#Realiza una rotacion entre 60/140 grados a todas las imagenes\n",
    "    transforms.Resize((299,299)),   \n",
    "    transforms.CenterCrop(180), #Se centra para eliminar las esquinas negras que aparecen\n",
    "    transforms.Resize((299,299)), #Se vuelve al tama침o 299x299\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((299,299)),\n",
    "    transforms.RandomRotation(degrees=(200, 340)), #Realiza una rotacion entre 200/340 grados a todas las imagenes\n",
    "    transforms.CenterCrop(180),\n",
    "    transforms.Resize((299,299)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "transform3 = transforms.Compose([\n",
    "    transforms.Resize((299,299)),\n",
    "    transforms.RandomHorizontalFlip(p=1), #Realiza un flip horizontal a todas las imagenes\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform4 = transforms.Compose([\n",
    "    transforms.Resize((299,299)),\n",
    "    transforms.RandomVerticalFlip(p=1), #Realiza un flip vertical a todas las imagenes\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Constructor del dataset.\n",
    "class goodBadDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "      super().__init__()\n",
    "      self.paths = image_paths\n",
    "      self.len = len(self.paths)\n",
    "      self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "      return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      path = self.paths[index]\n",
    "      image = Image.open(path)\n",
    "      image = self.transform(image)\n",
    "      label = 0 if 'Buenas-Reference' in path or 'Buena_Calidad' in path else 1 #Si en el path se encuentra el texto buena calidad se le coloca la etiqueta 0 si no un 1.\n",
    "      return (image,label,path) #Devuelve la imagen, su etiqueta y su path\n",
    "\n",
    "class goodBadDataset2(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "      super().__init__()\n",
    "      self.paths = image_paths\n",
    "      self.len = len(self.paths)\n",
    "      self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "      return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      path = self.paths[index]\n",
    "      image = Image.open(path)\n",
    "      image = self.transform(image)\n",
    "      label = 0 if 'Buena_Calidad' in path else 1 #Si en el path se encuentra el texto buena calidad se le coloca la etiqueta 0 si no un 1.\n",
    "      return (image,label,path) #Devuelve la imagen, su etiqueta y su path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de entreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10, verbose=True):\n",
    "    \n",
    "    model.train() #Se pone en estado de entreno\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    correct=0\n",
    "    \n",
    "    for batch_idx, (data, target,_) in enumerate(train_loader): #Por cada batch en el dataset\n",
    "    \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(reduction='sum')(output, target) \n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if verbose:\n",
    "            if len(data)==20:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "            else:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                    epoch,len(train_loader.dataset)-len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "        \n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item() #Si se ha predecido bien se suma.\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    accuracy=correct / len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss_v, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "    \n",
    " \n",
    "    \n",
    "    return loss_v,accuracy # Se devuelve la perdida y la precision conseguida\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval() #Se pone en estado de evaluacion\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target,_ in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += nn.CrossEntropyLoss(reduction='sum')(output, target) \n",
    "            \n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item() #Se suma si se ha predecido de forma correcta la etiqueta\n",
    " \n",
    "  \n",
    "    test_loss /= len(test_loader.dataset) #Se calcula la media de la perdida\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    accuracy=correct / len(test_loader.dataset)\n",
    "    return test_loss,accuracy # Se devuelve la perdida y la precision conseguida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definicion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def initialize_model2(num_classes):\n",
    "    #Los pesos si se entrenan\n",
    "    model_ft = models.inception_v3(weights=False) #Se carga el modelo inception_v3 sin sus pesos preentrenados.\n",
    "    # False en las redes convencionales, se entrenaran\n",
    "    set_parameter_requires_grad(model_ft, False)\n",
    "    # Se modifica la red auxiliar\n",
    "    num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "    model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Se modifica la red principal para que sea de dos clases con una salida softmax y coger la con mas probabilidad.\n",
    "    model_ft.dropout= nn.Dropout(0.5)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs,num_classes),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "    input_size=299 #Tama침o que usa la red\n",
    "    return model_ft,input_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def especifidad(MatrizConfusion):\n",
    "    espeficifadM=(MatrizConfusion[1][1]/(MatrizConfusion[1][1]+MatrizConfusion[1][0]))*100\n",
    "    return round(espeficifadM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(MatrizConfusion):\n",
    "    recallM=(MatrizConfusion[0][0]/(MatrizConfusion[0][0]+MatrizConfusion[0][1]))*100\n",
    "    return round(recallM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrecisionClasePositiva(MatrizConfusion):\n",
    "    PreP=(MatrizConfusion[0][0]/(MatrizConfusion[0][0]+MatrizConfusion[1][0]))*100\n",
    "    return round(PreP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrecisionClaseNegativa(MatrizConfusion):\n",
    "    PreN=(MatrizConfusion[1][1]/(MatrizConfusion[1][1]+MatrizConfusion[0][1]))*100\n",
    "    return round(PreN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactitud(MatrizConfusion):\n",
    "    exactitudM=((MatrizConfusion[0][0]+MatrizConfusion[1][1])/(MatrizConfusion[0][0]+MatrizConfusion[0][1]+MatrizConfusion[1][1]+MatrizConfusion[1][0]))*100\n",
    "    return round(exactitudM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entreno EUPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader de entreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imagenes de Buena calidad en el dataset de entreno 2540\n",
      "Total de imagenes de Mala calidad en el dataset de entreno 2605\n",
      "Longitud del dataset de entreno con transformaciones:  25725\n",
      "Cantidad de batches de 20 del dataloader de entreno con transformaciones:  1287\n"
     ]
    }
   ],
   "source": [
    "Buena_calidad=os.listdir('..\\Imagenes\\Dataset 2 modificado\\Entreno\\Buena_Calidad/')\n",
    "##Buena_calidad = list(filter(lambda x: x != 'Imagen', Buena_calidad))\n",
    "Buena_calidad= list(map(lambda  p: f\"..\\Imagenes\\Dataset 2 modificado\\Entreno\\Buena_Calidad/{p}\",Buena_calidad))\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "Mala_calidad=os.listdir('..\\Imagenes\\Dataset 2 modificado\\Entreno\\Mala_Calidad/')\n",
    "##Mala_calidad = list(filter(lambda x: x != 'Imagen', Mala_calidad))\n",
    "Mala_calidad= list(map(lambda  p: f\"..\\Imagenes\\Dataset 2 modificado\\Entreno\\Mala_Calidad/{p}\",Mala_calidad))\n",
    "\n",
    "print(\"Total de imagenes de Buena calidad en el dataset de entreno\", len(Buena_calidad))\n",
    "print(\"Total de imagenes de Mala calidad en el dataset de entreno\", len(Mala_calidad))\n",
    "\n",
    "img_files = Buena_calidad + Mala_calidad\n",
    "\n",
    "DatasetEntrenodMod=goodBadDataset(img_files,transform)+goodBadDataset(img_files,transform1)+goodBadDataset(img_files,transform2)+goodBadDataset(img_files,transform3)+goodBadDataset(img_files,transform4) # #Se genera un dataset de entreno con todas las transformaciones expuestas anteriormente a las imagenesy se pasan a tensor.\n",
    "\n",
    "\n",
    "#Randomizar las imagenes del dataset de entreno con las imagenes transformadas\n",
    "random_train_idx = np.random.choice(np.array(range(len(DatasetEntrenodMod))),replace=False, size=DatasetEntrenodMod.__len__())\n",
    "train_subset = Subset(DatasetEntrenodMod, random_train_idx)\n",
    "DataloaderEntrenoMod=DataLoader(train_subset,batch_size=20,shuffle=True)\n",
    "\n",
    "print(\"Longitud del dataset de entreno con transformaciones: \",len(DatasetEntrenodMod))\n",
    "print(\"Cantidad de batches de 20 del dataloader de entreno con transformaciones: \",len(DataloaderEntrenoMod))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader de validaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de imagenes de Buena calidad en el dataset de validacion 600\n",
      "Total de imagenes de Mala calidad en el dataset de validacion 600\n",
      "Longitud del dataset de validacion:  1200\n"
     ]
    }
   ],
   "source": [
    "ValBuena_calidad=os.listdir('..\\Imagenes\\Dataset 2 modificado\\Validacion\\Buena_Calidad/')\n",
    "##ValBuena_calidad = list(filter(lambda x: x != 'Imagen', ValBuena_calidad))\n",
    "ValBuena_calidad= list(map(lambda  p: f\"..\\Imagenes\\Dataset 2 modificado\\Validacion\\Buena_Calidad/{p}\",ValBuena_calidad))\n",
    "\n",
    "ValMala_calidad=os.listdir('..\\Imagenes\\Dataset 2 modificado\\Validacion\\Mala_Calidad/')\n",
    "##ValMala_calidad = list(filter(lambda x: x != 'Imagen', ValMala_calidad))\n",
    "ValMala_calidad= list(map(lambda  p: f\"..\\Imagenes\\Dataset 2 modificado\\Validacion\\Mala_Calidad/{p}\",ValMala_calidad))\n",
    "\n",
    "print(\"Total de imagenes de Buena calidad en el dataset de validacion\", len(ValBuena_calidad))\n",
    "print(\"Total de imagenes de Mala calidad en el dataset de validacion\", len(ValMala_calidad))\n",
    "\n",
    "Val_Imagenes = ValBuena_calidad + ValMala_calidad\n",
    "\n",
    "val_dsMod = goodBadDataset(Val_Imagenes, transform) #Se carga el dataset de validacion con solo imagenes pasadas a tensor y de tama침o 299x299\n",
    "val_dlMod = DataLoader(val_dsMod, batch_size=4) #Se carga el dataloader de validacion.\n",
    "\n",
    "print(\"Longitud del dataset de validacion: \",len(val_dsMod))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con datos Ampliados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t Conv2d_1a_3x3.conv.weight\n",
      "\t Conv2d_1a_3x3.bn.weight\n",
      "\t Conv2d_1a_3x3.bn.bias\n",
      "\t Conv2d_2a_3x3.conv.weight\n",
      "\t Conv2d_2a_3x3.bn.weight\n",
      "\t Conv2d_2a_3x3.bn.bias\n",
      "\t Conv2d_2b_3x3.conv.weight\n",
      "\t Conv2d_2b_3x3.bn.weight\n",
      "\t Conv2d_2b_3x3.bn.bias\n",
      "\t Conv2d_3b_1x1.conv.weight\n",
      "\t Conv2d_3b_1x1.bn.weight\n",
      "\t Conv2d_3b_1x1.bn.bias\n",
      "\t Conv2d_4a_3x3.conv.weight\n",
      "\t Conv2d_4a_3x3.bn.weight\n",
      "\t Conv2d_4a_3x3.bn.bias\n",
      "\t Mixed_5b.branch1x1.conv.weight\n",
      "\t Mixed_5b.branch1x1.bn.weight\n",
      "\t Mixed_5b.branch1x1.bn.bias\n",
      "\t Mixed_5b.branch5x5_1.conv.weight\n",
      "\t Mixed_5b.branch5x5_1.bn.weight\n",
      "\t Mixed_5b.branch5x5_1.bn.bias\n",
      "\t Mixed_5b.branch5x5_2.conv.weight\n",
      "\t Mixed_5b.branch5x5_2.bn.weight\n",
      "\t Mixed_5b.branch5x5_2.bn.bias\n",
      "\t Mixed_5b.branch3x3dbl_1.conv.weight\n",
      "\t Mixed_5b.branch3x3dbl_1.bn.weight\n",
      "\t Mixed_5b.branch3x3dbl_1.bn.bias\n",
      "\t Mixed_5b.branch3x3dbl_2.conv.weight\n",
      "\t Mixed_5b.branch3x3dbl_2.bn.weight\n",
      "\t Mixed_5b.branch3x3dbl_2.bn.bias\n",
      "\t Mixed_5b.branch3x3dbl_3.conv.weight\n",
      "\t Mixed_5b.branch3x3dbl_3.bn.weight\n",
      "\t Mixed_5b.branch3x3dbl_3.bn.bias\n",
      "\t Mixed_5b.branch_pool.conv.weight\n",
      "\t Mixed_5b.branch_pool.bn.weight\n",
      "\t Mixed_5b.branch_pool.bn.bias\n",
      "\t Mixed_5c.branch1x1.conv.weight\n",
      "\t Mixed_5c.branch1x1.bn.weight\n",
      "\t Mixed_5c.branch1x1.bn.bias\n",
      "\t Mixed_5c.branch5x5_1.conv.weight\n",
      "\t Mixed_5c.branch5x5_1.bn.weight\n",
      "\t Mixed_5c.branch5x5_1.bn.bias\n",
      "\t Mixed_5c.branch5x5_2.conv.weight\n",
      "\t Mixed_5c.branch5x5_2.bn.weight\n",
      "\t Mixed_5c.branch5x5_2.bn.bias\n",
      "\t Mixed_5c.branch3x3dbl_1.conv.weight\n",
      "\t Mixed_5c.branch3x3dbl_1.bn.weight\n",
      "\t Mixed_5c.branch3x3dbl_1.bn.bias\n",
      "\t Mixed_5c.branch3x3dbl_2.conv.weight\n",
      "\t Mixed_5c.branch3x3dbl_2.bn.weight\n",
      "\t Mixed_5c.branch3x3dbl_2.bn.bias\n",
      "\t Mixed_5c.branch3x3dbl_3.conv.weight\n",
      "\t Mixed_5c.branch3x3dbl_3.bn.weight\n",
      "\t Mixed_5c.branch3x3dbl_3.bn.bias\n",
      "\t Mixed_5c.branch_pool.conv.weight\n",
      "\t Mixed_5c.branch_pool.bn.weight\n",
      "\t Mixed_5c.branch_pool.bn.bias\n",
      "\t Mixed_5d.branch1x1.conv.weight\n",
      "\t Mixed_5d.branch1x1.bn.weight\n",
      "\t Mixed_5d.branch1x1.bn.bias\n",
      "\t Mixed_5d.branch5x5_1.conv.weight\n",
      "\t Mixed_5d.branch5x5_1.bn.weight\n",
      "\t Mixed_5d.branch5x5_1.bn.bias\n",
      "\t Mixed_5d.branch5x5_2.conv.weight\n",
      "\t Mixed_5d.branch5x5_2.bn.weight\n",
      "\t Mixed_5d.branch5x5_2.bn.bias\n",
      "\t Mixed_5d.branch3x3dbl_1.conv.weight\n",
      "\t Mixed_5d.branch3x3dbl_1.bn.weight\n",
      "\t Mixed_5d.branch3x3dbl_1.bn.bias\n",
      "\t Mixed_5d.branch3x3dbl_2.conv.weight\n",
      "\t Mixed_5d.branch3x3dbl_2.bn.weight\n",
      "\t Mixed_5d.branch3x3dbl_2.bn.bias\n",
      "\t Mixed_5d.branch3x3dbl_3.conv.weight\n",
      "\t Mixed_5d.branch3x3dbl_3.bn.weight\n",
      "\t Mixed_5d.branch3x3dbl_3.bn.bias\n",
      "\t Mixed_5d.branch_pool.conv.weight\n",
      "\t Mixed_5d.branch_pool.bn.weight\n",
      "\t Mixed_5d.branch_pool.bn.bias\n",
      "\t Mixed_6a.branch3x3.conv.weight\n",
      "\t Mixed_6a.branch3x3.bn.weight\n",
      "\t Mixed_6a.branch3x3.bn.bias\n",
      "\t Mixed_6a.branch3x3dbl_1.conv.weight\n",
      "\t Mixed_6a.branch3x3dbl_1.bn.weight\n",
      "\t Mixed_6a.branch3x3dbl_1.bn.bias\n",
      "\t Mixed_6a.branch3x3dbl_2.conv.weight\n",
      "\t Mixed_6a.branch3x3dbl_2.bn.weight\n",
      "\t Mixed_6a.branch3x3dbl_2.bn.bias\n",
      "\t Mixed_6a.branch3x3dbl_3.conv.weight\n",
      "\t Mixed_6a.branch3x3dbl_3.bn.weight\n",
      "\t Mixed_6a.branch3x3dbl_3.bn.bias\n",
      "\t Mixed_6b.branch1x1.conv.weight\n",
      "\t Mixed_6b.branch1x1.bn.weight\n",
      "\t Mixed_6b.branch1x1.bn.bias\n",
      "\t Mixed_6b.branch7x7_1.conv.weight\n",
      "\t Mixed_6b.branch7x7_1.bn.weight\n",
      "\t Mixed_6b.branch7x7_1.bn.bias\n",
      "\t Mixed_6b.branch7x7_2.conv.weight\n",
      "\t Mixed_6b.branch7x7_2.bn.weight\n",
      "\t Mixed_6b.branch7x7_2.bn.bias\n",
      "\t Mixed_6b.branch7x7_3.conv.weight\n",
      "\t Mixed_6b.branch7x7_3.bn.weight\n",
      "\t Mixed_6b.branch7x7_3.bn.bias\n",
      "\t Mixed_6b.branch7x7dbl_1.conv.weight\n",
      "\t Mixed_6b.branch7x7dbl_1.bn.weight\n",
      "\t Mixed_6b.branch7x7dbl_1.bn.bias\n",
      "\t Mixed_6b.branch7x7dbl_2.conv.weight\n",
      "\t Mixed_6b.branch7x7dbl_2.bn.weight\n",
      "\t Mixed_6b.branch7x7dbl_2.bn.bias\n",
      "\t Mixed_6b.branch7x7dbl_3.conv.weight\n",
      "\t Mixed_6b.branch7x7dbl_3.bn.weight\n",
      "\t Mixed_6b.branch7x7dbl_3.bn.bias\n",
      "\t Mixed_6b.branch7x7dbl_4.conv.weight\n",
      "\t Mixed_6b.branch7x7dbl_4.bn.weight\n",
      "\t Mixed_6b.branch7x7dbl_4.bn.bias\n",
      "\t Mixed_6b.branch7x7dbl_5.conv.weight\n",
      "\t Mixed_6b.branch7x7dbl_5.bn.weight\n",
      "\t Mixed_6b.branch7x7dbl_5.bn.bias\n",
      "\t Mixed_6b.branch_pool.conv.weight\n",
      "\t Mixed_6b.branch_pool.bn.weight\n",
      "\t Mixed_6b.branch_pool.bn.bias\n",
      "\t Mixed_6c.branch1x1.conv.weight\n",
      "\t Mixed_6c.branch1x1.bn.weight\n",
      "\t Mixed_6c.branch1x1.bn.bias\n",
      "\t Mixed_6c.branch7x7_1.conv.weight\n",
      "\t Mixed_6c.branch7x7_1.bn.weight\n",
      "\t Mixed_6c.branch7x7_1.bn.bias\n",
      "\t Mixed_6c.branch7x7_2.conv.weight\n",
      "\t Mixed_6c.branch7x7_2.bn.weight\n",
      "\t Mixed_6c.branch7x7_2.bn.bias\n",
      "\t Mixed_6c.branch7x7_3.conv.weight\n",
      "\t Mixed_6c.branch7x7_3.bn.weight\n",
      "\t Mixed_6c.branch7x7_3.bn.bias\n",
      "\t Mixed_6c.branch7x7dbl_1.conv.weight\n",
      "\t Mixed_6c.branch7x7dbl_1.bn.weight\n",
      "\t Mixed_6c.branch7x7dbl_1.bn.bias\n",
      "\t Mixed_6c.branch7x7dbl_2.conv.weight\n",
      "\t Mixed_6c.branch7x7dbl_2.bn.weight\n",
      "\t Mixed_6c.branch7x7dbl_2.bn.bias\n",
      "\t Mixed_6c.branch7x7dbl_3.conv.weight\n",
      "\t Mixed_6c.branch7x7dbl_3.bn.weight\n",
      "\t Mixed_6c.branch7x7dbl_3.bn.bias\n",
      "\t Mixed_6c.branch7x7dbl_4.conv.weight\n",
      "\t Mixed_6c.branch7x7dbl_4.bn.weight\n",
      "\t Mixed_6c.branch7x7dbl_4.bn.bias\n",
      "\t Mixed_6c.branch7x7dbl_5.conv.weight\n",
      "\t Mixed_6c.branch7x7dbl_5.bn.weight\n",
      "\t Mixed_6c.branch7x7dbl_5.bn.bias\n",
      "\t Mixed_6c.branch_pool.conv.weight\n",
      "\t Mixed_6c.branch_pool.bn.weight\n",
      "\t Mixed_6c.branch_pool.bn.bias\n",
      "\t Mixed_6d.branch1x1.conv.weight\n",
      "\t Mixed_6d.branch1x1.bn.weight\n",
      "\t Mixed_6d.branch1x1.bn.bias\n",
      "\t Mixed_6d.branch7x7_1.conv.weight\n",
      "\t Mixed_6d.branch7x7_1.bn.weight\n",
      "\t Mixed_6d.branch7x7_1.bn.bias\n",
      "\t Mixed_6d.branch7x7_2.conv.weight\n",
      "\t Mixed_6d.branch7x7_2.bn.weight\n",
      "\t Mixed_6d.branch7x7_2.bn.bias\n",
      "\t Mixed_6d.branch7x7_3.conv.weight\n",
      "\t Mixed_6d.branch7x7_3.bn.weight\n",
      "\t Mixed_6d.branch7x7_3.bn.bias\n",
      "\t Mixed_6d.branch7x7dbl_1.conv.weight\n",
      "\t Mixed_6d.branch7x7dbl_1.bn.weight\n",
      "\t Mixed_6d.branch7x7dbl_1.bn.bias\n",
      "\t Mixed_6d.branch7x7dbl_2.conv.weight\n",
      "\t Mixed_6d.branch7x7dbl_2.bn.weight\n",
      "\t Mixed_6d.branch7x7dbl_2.bn.bias\n",
      "\t Mixed_6d.branch7x7dbl_3.conv.weight\n",
      "\t Mixed_6d.branch7x7dbl_3.bn.weight\n",
      "\t Mixed_6d.branch7x7dbl_3.bn.bias\n",
      "\t Mixed_6d.branch7x7dbl_4.conv.weight\n",
      "\t Mixed_6d.branch7x7dbl_4.bn.weight\n",
      "\t Mixed_6d.branch7x7dbl_4.bn.bias\n",
      "\t Mixed_6d.branch7x7dbl_5.conv.weight\n",
      "\t Mixed_6d.branch7x7dbl_5.bn.weight\n",
      "\t Mixed_6d.branch7x7dbl_5.bn.bias\n",
      "\t Mixed_6d.branch_pool.conv.weight\n",
      "\t Mixed_6d.branch_pool.bn.weight\n",
      "\t Mixed_6d.branch_pool.bn.bias\n",
      "\t Mixed_6e.branch1x1.conv.weight\n",
      "\t Mixed_6e.branch1x1.bn.weight\n",
      "\t Mixed_6e.branch1x1.bn.bias\n",
      "\t Mixed_6e.branch7x7_1.conv.weight\n",
      "\t Mixed_6e.branch7x7_1.bn.weight\n",
      "\t Mixed_6e.branch7x7_1.bn.bias\n",
      "\t Mixed_6e.branch7x7_2.conv.weight\n",
      "\t Mixed_6e.branch7x7_2.bn.weight\n",
      "\t Mixed_6e.branch7x7_2.bn.bias\n",
      "\t Mixed_6e.branch7x7_3.conv.weight\n",
      "\t Mixed_6e.branch7x7_3.bn.weight\n",
      "\t Mixed_6e.branch7x7_3.bn.bias\n",
      "\t Mixed_6e.branch7x7dbl_1.conv.weight\n",
      "\t Mixed_6e.branch7x7dbl_1.bn.weight\n",
      "\t Mixed_6e.branch7x7dbl_1.bn.bias\n",
      "\t Mixed_6e.branch7x7dbl_2.conv.weight\n",
      "\t Mixed_6e.branch7x7dbl_2.bn.weight\n",
      "\t Mixed_6e.branch7x7dbl_2.bn.bias\n",
      "\t Mixed_6e.branch7x7dbl_3.conv.weight\n",
      "\t Mixed_6e.branch7x7dbl_3.bn.weight\n",
      "\t Mixed_6e.branch7x7dbl_3.bn.bias\n",
      "\t Mixed_6e.branch7x7dbl_4.conv.weight\n",
      "\t Mixed_6e.branch7x7dbl_4.bn.weight\n",
      "\t Mixed_6e.branch7x7dbl_4.bn.bias\n",
      "\t Mixed_6e.branch7x7dbl_5.conv.weight\n",
      "\t Mixed_6e.branch7x7dbl_5.bn.weight\n",
      "\t Mixed_6e.branch7x7dbl_5.bn.bias\n",
      "\t Mixed_6e.branch_pool.conv.weight\n",
      "\t Mixed_6e.branch_pool.bn.weight\n",
      "\t Mixed_6e.branch_pool.bn.bias\n",
      "\t Mixed_7a.branch3x3_1.conv.weight\n",
      "\t Mixed_7a.branch3x3_1.bn.weight\n",
      "\t Mixed_7a.branch3x3_1.bn.bias\n",
      "\t Mixed_7a.branch3x3_2.conv.weight\n",
      "\t Mixed_7a.branch3x3_2.bn.weight\n",
      "\t Mixed_7a.branch3x3_2.bn.bias\n",
      "\t Mixed_7a.branch7x7x3_1.conv.weight\n",
      "\t Mixed_7a.branch7x7x3_1.bn.weight\n",
      "\t Mixed_7a.branch7x7x3_1.bn.bias\n",
      "\t Mixed_7a.branch7x7x3_2.conv.weight\n",
      "\t Mixed_7a.branch7x7x3_2.bn.weight\n",
      "\t Mixed_7a.branch7x7x3_2.bn.bias\n",
      "\t Mixed_7a.branch7x7x3_3.conv.weight\n",
      "\t Mixed_7a.branch7x7x3_3.bn.weight\n",
      "\t Mixed_7a.branch7x7x3_3.bn.bias\n",
      "\t Mixed_7a.branch7x7x3_4.conv.weight\n",
      "\t Mixed_7a.branch7x7x3_4.bn.weight\n",
      "\t Mixed_7a.branch7x7x3_4.bn.bias\n",
      "\t Mixed_7b.branch1x1.conv.weight\n",
      "\t Mixed_7b.branch1x1.bn.weight\n",
      "\t Mixed_7b.branch1x1.bn.bias\n",
      "\t Mixed_7b.branch3x3_1.conv.weight\n",
      "\t Mixed_7b.branch3x3_1.bn.weight\n",
      "\t Mixed_7b.branch3x3_1.bn.bias\n",
      "\t Mixed_7b.branch3x3_2a.conv.weight\n",
      "\t Mixed_7b.branch3x3_2a.bn.weight\n",
      "\t Mixed_7b.branch3x3_2a.bn.bias\n",
      "\t Mixed_7b.branch3x3_2b.conv.weight\n",
      "\t Mixed_7b.branch3x3_2b.bn.weight\n",
      "\t Mixed_7b.branch3x3_2b.bn.bias\n",
      "\t Mixed_7b.branch3x3dbl_1.conv.weight\n",
      "\t Mixed_7b.branch3x3dbl_1.bn.weight\n",
      "\t Mixed_7b.branch3x3dbl_1.bn.bias\n",
      "\t Mixed_7b.branch3x3dbl_2.conv.weight\n",
      "\t Mixed_7b.branch3x3dbl_2.bn.weight\n",
      "\t Mixed_7b.branch3x3dbl_2.bn.bias\n",
      "\t Mixed_7b.branch3x3dbl_3a.conv.weight\n",
      "\t Mixed_7b.branch3x3dbl_3a.bn.weight\n",
      "\t Mixed_7b.branch3x3dbl_3a.bn.bias\n",
      "\t Mixed_7b.branch3x3dbl_3b.conv.weight\n",
      "\t Mixed_7b.branch3x3dbl_3b.bn.weight\n",
      "\t Mixed_7b.branch3x3dbl_3b.bn.bias\n",
      "\t Mixed_7b.branch_pool.conv.weight\n",
      "\t Mixed_7b.branch_pool.bn.weight\n",
      "\t Mixed_7b.branch_pool.bn.bias\n",
      "\t Mixed_7c.branch1x1.conv.weight\n",
      "\t Mixed_7c.branch1x1.bn.weight\n",
      "\t Mixed_7c.branch1x1.bn.bias\n",
      "\t Mixed_7c.branch3x3_1.conv.weight\n",
      "\t Mixed_7c.branch3x3_1.bn.weight\n",
      "\t Mixed_7c.branch3x3_1.bn.bias\n",
      "\t Mixed_7c.branch3x3_2a.conv.weight\n",
      "\t Mixed_7c.branch3x3_2a.bn.weight\n",
      "\t Mixed_7c.branch3x3_2a.bn.bias\n",
      "\t Mixed_7c.branch3x3_2b.conv.weight\n",
      "\t Mixed_7c.branch3x3_2b.bn.weight\n",
      "\t Mixed_7c.branch3x3_2b.bn.bias\n",
      "\t Mixed_7c.branch3x3dbl_1.conv.weight\n",
      "\t Mixed_7c.branch3x3dbl_1.bn.weight\n",
      "\t Mixed_7c.branch3x3dbl_1.bn.bias\n",
      "\t Mixed_7c.branch3x3dbl_2.conv.weight\n",
      "\t Mixed_7c.branch3x3dbl_2.bn.weight\n",
      "\t Mixed_7c.branch3x3dbl_2.bn.bias\n",
      "\t Mixed_7c.branch3x3dbl_3a.conv.weight\n",
      "\t Mixed_7c.branch3x3dbl_3a.bn.weight\n",
      "\t Mixed_7c.branch3x3dbl_3a.bn.bias\n",
      "\t Mixed_7c.branch3x3dbl_3b.conv.weight\n",
      "\t Mixed_7c.branch3x3dbl_3b.bn.weight\n",
      "\t Mixed_7c.branch3x3dbl_3b.bn.bias\n",
      "\t Mixed_7c.branch_pool.conv.weight\n",
      "\t Mixed_7c.branch_pool.bn.weight\n",
      "\t Mixed_7c.branch_pool.bn.bias\n",
      "\t fc.0.weight\n",
      "\t fc.0.bias\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2048, out_features=2, bias=True)\n",
      "  (1): Dropout(p=0.3, inplace=False)\n",
      "  (2): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "modelModft,_= initialize_model2(2)\n",
    "modelMod=modelModft.to(device)\n",
    "\n",
    "modelModft.aux_logits=False\n",
    "\n",
    "params_to_update = modelModft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in modelModft.named_parameters():\n",
    "    \n",
    "    if (param.requires_grad == True) & (not ('AuxLogits' in name)):\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "\n",
    "print(modelModft.fc)\n",
    "\n",
    "\n",
    "parametrosMod=sum(p.numel() for p in params_to_update)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entreno con Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Variables para el seguimiento del early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # N칰mero de 칠pocas sin mejora antes de detener el entrenamiento\n",
    "counter = 0\n",
    "\n",
    "epochs=40\n",
    "lr=0.001\n",
    "EpocasTotalesEUPV = epochs\n",
    "\n",
    "acurracytrainEUPV = []\n",
    "acurracvalEUPV = []\n",
    "losstrainEUPV = []\n",
    "lossvalEUPV = []\n",
    "\n",
    "\n",
    "print(\"Parameters \",parametrosMod)\n",
    "optimizer = optim.Adam(params_to_update, lr=lr)\n",
    "\n",
    "\n",
    "# Loop de entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    # Entrenamiento\n",
    "    train_loss, train_accuracy = train(modelMod, device, DataloaderEntrenoMod, optimizer, epoch, verbose=True)\n",
    "    \n",
    "    # Evaluaci칩n en el conjunto de validaci칩n\n",
    "    val_loss, val_accuracy = test(modelMod, device, val_dlMod)\n",
    "    \n",
    "    acurracytrainEUPV.append(train_accuracy)\n",
    "    acurracvalEUPV.append(val_accuracy)\n",
    "    losstrainEUPV.append(train_loss)\n",
    "    lossvalEUPV.append(val_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Verifica si la p칠rdida de validaci칩n ha mejorado\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        counter = 0  # Reinicia el contador de paciencia\n",
    "        ruta_base = r'..\\Pesos\\Dataset 2 Modificado\\modeltranformaciones'\n",
    "        # Convierte el entero a una cadena y concatena todo\n",
    "        PATH = f\"{ruta_base}Epoca{epoch}.pt\"      \n",
    "        torch.save(modelMod.state_dict(), PATH)\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    # Aplica early stopping si se alcanza la paciencia m치xima\n",
    "    if counter >= patience:  # Ajusta el n칰mero de 칠pocas de paciencia seg칰n tus necesidades\n",
    "        print(f'Early stopping after epoch {epoch} (Best epoch: {best_epoch}).')\n",
    "        EpocasTotalesEUPV = epoch + 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripcion de como Cargar un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe tener una instancia del modelo creada, para ello primero se tiene que definir el modelo. Con el siguiente codigo se define el modelo utilizado:\n",
    "\n",
    "    def initialize_model2(num_classes):\n",
    "        #Los pesos si se entrenan\n",
    "        model_ft = models.inception_v3(weights=False) #Se carga el modelo inception_v3 sin sus pesos preentrenados.\n",
    "        # False en las redes convencionales, se entrenaran\n",
    "        set_parameter_requires_grad(model_ft, False)\n",
    "        # Se modifica la red auxiliar\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        #Se modifica la red principal para que sea de dos clases con una salida softmax y coger la con mas probabilidad.\n",
    "        model_ft.dropout= nn.Dropout(0.5)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs,num_classes),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        input_size=299 #Tama침o que usa la red\n",
    "        return model_ft,input_size\n",
    "    \n",
    "\n",
    "\n",
    "Y con la siguiente funcion:\n",
    "\n",
    "    def set_parameter_requires_grad(model, feature_extracting):\n",
    "        if feature_extracting:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            \n",
    "\n",
    "Y a continuacion se instancia un modelo. Para hacer eso se deben utilizar las siguientes lineas de codigo:\n",
    "Device cpu si no se dispone de cuda y si no  device = torch.device(\"cuda:0\") con el indice deseado.\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    Modeloft,_= initialize_model2(2)\n",
    "    Modelo=Modeloft.to(device)\n",
    "\n",
    "    #Para inutilizar la capa auxiliar.\n",
    "    Modeloft.aux_logits=False\n",
    "\n",
    "\n",
    "\n",
    "Para poder cargar los pesos del modelo se hace con la siguiente linea:\n",
    "\n",
    "    PATH = '..\\Pesos\\modeltranformaciones.pt'\n",
    "    Modelo.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "\n",
    "\n",
    "Y para finalizar para poder usarlo en modo prediccion hay que ponerlo en modo eval:\n",
    "\n",
    "    Modelo.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puntuaciones a los diferentes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.image_paths[index]\n",
    "        image = Image.open(path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image,path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuento y Valorado de imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modelos = [\n",
    "    \"Dataset 2 Modificado\\modeltranformacionesEpoca38\",\n",
    "    \"Dataset unidos\\modeltranformacionesEpoca26\"  \n",
    "]\n",
    "RutasImagenes = [\n",
    "    \"../Imagenes/Pruebas/challenging-60/\",\n",
    "    \"../Imagenes/Pruebas/Paired/underwater_scenes/Buena_Calidad/\",\n",
    "    \"../Imagenes/Pruebas/Paired/underwater_scenes/Mala_Calidad/\",\n",
    "    \"../Imagenes/Pruebas/Paired/underwater_imagenet/Buena_Calidad/\",\n",
    "    \"../Imagenes/Pruebas/Paired/underwater_imagenet/Mala_Calidad/\",\n",
    "    \"../Imagenes/Pruebas/Paired/underwater_dark/Buena_Calidad/\",\n",
    "    \"../Imagenes/Pruebas/Paired/underwater_dark/Mala_Calidad/\",\n",
    "    \"../Imagenes/Dataset 2 modificado/Validacion/Buena_Calidad/\",\n",
    "    \"../Imagenes/Dataset 2 modificado/Validacion/Mala_Calidad/\",\n",
    "    \"../Imagenes/Dataset 1/Validacion/Buenas-Reference/\",\n",
    "    \"../Imagenes/Dataset 1/Validacion/Malas-Raw/\" \n",
    "]\n",
    "carpetasMo = [\n",
    "    \"Dataset 2 mod\",\n",
    "    \"Dataset Unido\"\n",
    "]\n",
    "archivosPunt = [\n",
    "    \"PuntuacionesChallenging\",\n",
    "    \"PuntuacionesPairedScenesGood\",\n",
    "    \"PuntuacionesPairedScenesBad\",\n",
    "    \"PuntuacionesPairedNetGood\",\n",
    "    \"PuntuacionesPairedNetBad\",\n",
    "    \"PuntuacionesPairedDarkGood\",\n",
    "    \"PuntuacionesPairedDarkBad\",\n",
    "    \"Dataset 2 modificado Good\",\n",
    "    \"Dataset 2 modificado Bad\",\n",
    "    \"Dataset 1 Good\",\n",
    "    \"Dataset 1 Bad\"     \n",
    "]\n",
    "NombreColImagenes = [\n",
    "    \"Challenging\",\n",
    "    \"Scenes Buenas\",\n",
    "    \"Scenes Malas\",\n",
    "    \"Net Buenas\",\n",
    "    \"Net Malas\",\n",
    "    \"Dark Buenas\",\n",
    "    \"Dark Malas\",\n",
    "    \"Dataset 2 Buenas\",\n",
    "    \"Dataset 2 Malas\",\n",
    "    \"Dataset 1 Buenas\",\n",
    "    \"Dataset 1 Malas\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Imagenes/Pruebas/Paired/underwater_scenes/Buena_Calidad/\n"
     ]
    }
   ],
   "source": [
    "print(RutasImagenes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\joseb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado desde ..\\Pesos\\Dataset 2 Modificado\\modeltranformacionesEpoca38.pt\n",
      "Longitud:  60\n",
      "Longitud:  2185\n",
      "Longitud:  2185\n",
      "Longitud:  3700\n",
      "Longitud:  3700\n",
      "Longitud:  5550\n",
      "Longitud:  5550\n",
      "Longitud:  600\n",
      "Longitud:  600\n",
      "Longitud:  190\n",
      "Longitud:  190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\joseb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado desde ..\\Pesos\\Dataset unidos\\modeltranformacionesEpoca26.pt\n",
      "Longitud:  60\n",
      "Longitud:  2185\n",
      "Longitud:  2185\n",
      "Longitud:  3700\n",
      "Longitud:  3700\n",
      "Longitud:  5550\n",
      "Longitud:  5550\n",
      "Longitud:  600\n",
      "Longitud:  600\n",
      "Longitud:  190\n",
      "Longitud:  190\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modeloIndice = 0\n",
    "\n",
    "for ruta in Modelos:\n",
    "    # Crear un nuevo modelo\n",
    "    Modeloft, _ = initialize_model2(2)\n",
    "    Modelo = Modeloft.to(device)\n",
    "    Modeloft.aux_logits = False\n",
    "\n",
    "    # Construir la ruta completa al archivo\n",
    "    PATH = f'..\\\\Pesos\\\\{ruta}.pt'\n",
    "\n",
    "    # Cargar los pesos del modelo desde el archivo\n",
    "    Modelo.load_state_dict(torch.load(PATH))\n",
    "    Modelo.eval()\n",
    "    print(f\"Modelo cargado desde {PATH}\")\n",
    "    dfRecuentoImg = pd.DataFrame(columns=['Imagenes','Buenas', 'Malas']) \n",
    "      \n",
    "    indice = 0\n",
    "    for carpeta in RutasImagenes:\n",
    "        \n",
    "        Imagenes=os.listdir(carpeta)\n",
    "        Imagenes= list(map(lambda  p: f\"{carpeta}{p}\",Imagenes))\n",
    "        print(\"Longitud: \" , len(Imagenes))\n",
    "        Dataset = UnlabeledImageDataset(Imagenes,transform)\n",
    "        buena_calidad = 0\n",
    "        mala_calidad = 0\n",
    "        dfMetricasImagenes = pd.DataFrame(columns=['Nombre Imagen', '% Buena calidad'])\n",
    "        for i in range(len(Dataset)):\n",
    "            img,path= Dataset.__getitem__(i)\n",
    "            tensor_imagen = img.unsqueeze(0)\n",
    "            tensor_imagen = tensor_imagen.to(device)\n",
    "            with torch.no_grad():\n",
    "                prediccion = Modelo(tensor_imagen)\n",
    "                clase_mas_probable = torch.argmax(prediccion, dim=1)\n",
    "                if clase_mas_probable == 0:\n",
    "                    buena_calidad = buena_calidad + 1\n",
    "                    destino = f'..\\\\Valores de calidad\\\\{carpetasMo[modeloIndice]}\\\\{NombreColImagenes[indice]}\\\\Buenas'\n",
    "                else:\n",
    "                    mala_calidad = mala_calidad + 1\n",
    "                    destino = f'..\\\\Valores de calidad\\\\{carpetasMo[modeloIndice]}\\\\{NombreColImagenes[indice]}\\\\Malas'\n",
    "    \n",
    "                nombre_archivo = os.path.basename(path)\n",
    "                destino_path = os.path.join(destino, nombre_archivo)\n",
    "                shutil.copyfile(path, destino_path)\n",
    "\n",
    "                #Prediccion[0][1] es el % de mala calidad que tiene la imagen\n",
    "                dfMetricasImagenes.loc[i] = [path, '{:.6f}'.format(prediccion[0][0].item()* 100)]\n",
    "        dfMetricasImagenes.to_csv(f'..\\\\Valores de calidad\\\\{carpetasMo[modeloIndice]}\\\\{archivosPunt[indice]}.csv', index=False)\n",
    "        dfRecuentoImg.loc[indice] = [NombreColImagenes[indice],buena_calidad,mala_calidad]        \n",
    "        indice = indice + 1\n",
    "    dfRecuentoImg.to_csv(f'..\\\\Valores de calidad\\\\{carpetasMo[modeloIndice]}\\\\recuento{carpetasMo[modeloIndice]}.csv', index=False)    \n",
    "    modeloIndice = modeloIndice + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
